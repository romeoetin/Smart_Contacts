{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCsFmIMbStx9"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myzQnLMOJ91"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C1-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqkGkF3Dtwqt"
      },
      "source": [
        "# Lab: Compare N-Gram Models and Transformer Language Models\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_3_compare_n_gram_models_and_transformer_language_models.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Compare the generations of n-gram and transformer language models.\n",
        "\n",
        "30 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md4FSU7h9dQR"
      },
      "source": [
        "## Overview\n",
        "\n",
        "So far, you have encountered two methods to estimate the probability distribution over the next token given a prompt. In the first lab, you manually assigned probabilities to lists of candidate tokens, and in the second lab, you used n-gram counts to build a language model.\n",
        "\n",
        "As you have seen in the previous labs, neither of these methods are ideal. Assigning probabilities manually, on the one hand, would be too time-intensive in practice and it would be impossible to list all possible prompts. The n-gram models, on the other hand, often produced generations that did not make sense due to their short context window, or the model failed to generate a sequence of tokens at all due to the sparsity in the dataset.\n",
        "\n",
        "In this lab, you will experiment with a more advanced language model based on the **transformer architecture**. The transformer architecture is an example of a neural network model, a class of sophisticated machine learning models that can learn very complex patterns from data. Transformers provide the foundation for modern large language models. These models are much better at producing coherent responses to arbitrary prompts than n-gram models.\n",
        "\n",
        "You will explore this yourself by comparing generations using your n-gram model to generations from a transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8u6btAn9jc4"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will understand:\n",
        "* How the probability distributions predicted by n-gram models and transformer models differ.\n",
        "* How the generations based on these probability distributions differ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIhcVxx0foVo"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "In this lab, you will not have to write any new code but instead you will interact with two language models: the Gemma-1B transformer model and a trigram model.\n",
        "\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the transformer model Gemma-1B and the trigram language model from the previous lab.\n",
        "* Observe how the probability distribution over the next token varies for the two models.\n",
        "* Explore how the generations of the two models differ.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (▶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ⌘+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose **Runtime** → **Run before** from the menu above (or use the keyboard combination Ctrl/⌘ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opdrrb6ITB4I"
      },
      "source": [
        "## Using Colab with a GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZjWayynW6nu"
      },
      "source": [
        "A **GPU** is a special type of hardware that can significantly speed up some types of computations of machine learning models. Several of the activities in this lab will also run a lot faster if you run them on a GPU.\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "5.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQQlDe0hL8AY"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily interact with the `ai_foundations` package, which has been specifically developed for this course. In the background, this package uses the [`gemma`](https://github.com/google-deepmind/gemma) package to load and prompt the Gemma-1B model and the [`plotly`](https://plotly.com/python/) package for creating visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VHl2mOBczMh7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install orbax-checkpoint==0.11.21 jax[cuda12]==0.6.2\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "# Packages used.\n",
        "import os # For setting a variable needed to load the model onto the GPU.\n",
        "import pandas as pd # For loading the Africa Galore dataset.\n",
        "\n",
        "# Functions for clearing outputs and formatting.\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "# Functions for generating texts with a language model, visualizing probability\n",
        "# distributions, and loading an n-gram model.\n",
        "from ai_foundations import generation\n",
        "from ai_foundations import visualizations\n",
        "from ai_foundations.ngram import model as ngram_model\n",
        "\n",
        "# Set the full GPU memory usage for JAX.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJBG_G9G4_Sv"
      },
      "source": [
        "## Load the models\n",
        "\n",
        "As a preparation for the comparisons between the n-gram model and the transformer model, the following cell loads the Africa Galore dataset and initializes a trigram model whose probabilities are estimated from the n-gram counts in that dataset. It also loads the Gemma-1B model.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ℹ️ Info: The Gemma-1B model**\n",
        ">\n",
        ">The transformer model you will interact with in this lab is the _Gemma-1B_ model that has been developed and trained by Google [1]. You will learn more about what it means to train a model in later parts of this course but essentially, the process of **training** it to teach the model a specific task using a dataset. In the case of a language model, the task is to predict the next token based on a prompt. When you estimated the probabilities of the n-gram models using the counts in a corpus, you also trained the model. The output of the training process are **parameters** of the model. These parameters guide the model to perform whatever task it was trained to do. In case of the n-gram language model, the model parameters were the conditional probabilities. In case of transformer models, the parameters are a (often very large) collection of numbers that determine the model behavior. A single one of these numbers does not mean anything but in combination, these numbers capture many patterns about language. In the case of Gemma-1B, there are around 1 billion such numbers, which gives this model its name.\n",
        "------\n",
        "\n",
        "<br />\n",
        "\n",
        "Run the following cell to load the two models. Note that loading the Gemma model may take up to a minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5faCFaib4-Z4"
      },
      "outputs": [],
      "source": [
        "# Load the Africa Galore dataset.\n",
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"]\n",
        "print(f\"Loaded Africa Galore dataset with {len(dataset)} paragraphs.\\n\")\n",
        "\n",
        "# Load a trigram model whose probabilities have been estimated using the\n",
        "# Africa Galore dataset.\n",
        "trigram_model = ngram_model.NGramModel(dataset, 3)\n",
        "print(\"Loaded trigram model.\\n\")\n",
        "\n",
        "print(\"Loading Gemma-1B model...\")\n",
        "gemma_model = generation.load_gemma()\n",
        "print(\"Loaded Gemma-1B model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtw_fxjErHlr"
      },
      "source": [
        "## Comparing model outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuDhg66vrKyn"
      },
      "source": [
        "Now that the models have been loaded, you can compare their generations. Models are usually evaluated against many criteria and which criteria are deemed most relevant depends on the task you are trying to solve. For example, if you are developing a chatbot that aims to provide information, then it is important the model provides accurate information. If on the other hand, you are developing a model for more creative tasks, then it may be more important that the generations are very diverse and not repetitive.\n",
        "\n",
        "In this lab, focus on the following evaluation criteria:\n",
        "1. **Fluency**: Does it read naturally? Grammatical mistakes, for example, would lower the fluency. Similarly, even if sentences are grammatical, if they go on and on, they may be difficult to comprehend.\n",
        "2. **Coherence**: Does it make logical sense and stay on topic? As language models are predicting one token at a time, the end of a generation may be about a different topic than its beginning.\n",
        "3. **Relevance**: Does it fit the context or prompt? A model might generate a response composed of random-looking tokens that don't constitute a proper answer.\n",
        "4. **Bias**: Does the output promote inequalities? Language models are trained on human-written data that likely include biases and promote stereotypes. You may observe very stereotypical outputs that could promote inequalities in the generations of a model.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpCVDaGF95Hj"
      },
      "source": [
        "### Predict the next token\n",
        "\n",
        "As a first investigation, generate a single token for the prompt \"Jide was hungry so she went looking for.\" Then vary the prompt and see how the predictions change. You can generate the token by entering a prompt and running the cell below.\n",
        "\n",
        "Evaluate whether the predicted token always makes sense in the context. Also note whether both models are able to predict the next token for arbitrary prompts. You can even enter a prompt in another language if you speak one and see how the model responds.\n",
        "\n",
        "Note that the tokens in many transformer models may be only a character or a part of a word. As such, the generations may sometimes end with tokens that are only the beginning of a word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3wf_FBdgRKmP"
      },
      "outputs": [],
      "source": [
        "# @title Compute the next token for a prompt\n",
        "\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
        "\n",
        "output_text_transformer, _, _ = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt, max_new_tokens=1, loaded_model=gemma_model\n",
        "    )\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
        "\n",
        "output_text_ngram = trigram_model.generate(1, prompt)\n",
        "print(f\"Generation by trigram model:\\n{output_text_ngram}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nV9tK-R42Ja"
      },
      "source": [
        "### Visualize the probability distribution over the predicted next token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmG3D2U6wos4"
      },
      "source": [
        "To get a better idea of what the model will be likely to generate, it can be useful to visualize the probability distribution over the next token.\n",
        "\n",
        "Run the following cell to plot the probability distributions over the next token for the prompt below. Each bar of the plots represents a different token, and its height corresponds to the probability assigned to that token by the model. The taller the bar, the more likely the model would choose that token for generating a sequence.\n",
        "\n",
        "Note that in order to make the plots more compact, they only show the probabilities of the 30 tokens with the highest probabilities. The transformer model assigns a non-zero probability to many more tokens, so the probabilities shown in this plot will likely not sum to 1.\n",
        "\n",
        "Run the cell and examine the distribution over the tokens below. Would including all these tokens result in fluent texts? Does the distribution of probabilities across tokens make sense? Repeat this process for several different prompts to get a sense of how the distributions by the trigram model compare to the distributions by Gemma-1B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iNbt0z7AwnzL"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the probability distributions\n",
        "\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
        "\n",
        "output_text_transformer, next_token_logits, tokenizer = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt, max_new_tokens=1, loaded_model=gemma_model\n",
        "    )\n",
        ")\n",
        "\n",
        "display(HTML(\"<h3>Gemma-1B</h3>\"))\n",
        "\n",
        "# Visualize the Gemma-1B probabilities.\n",
        "visualizations.plot_next_token(\n",
        "    next_token_logits,\n",
        "    prompt=prompt,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "display(HTML(\"<h3>Trigram model</h3>\"))\n",
        "\n",
        "# Visualize the trigram probabilities.\n",
        "context_ngram = tuple(prompt.split(\" \")[-2:])\n",
        "if context_ngram in trigram_model.probabilities:\n",
        "    visualizations.plot_next_token(\n",
        "        trigram_model.probabilities[context_ngram], prompt=prompt\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"The trigram model does not make any predictions for the prompt\"\n",
        "        f\" \\\"{prompt}\\\" since the bigram \\\"{' '.join(context_ngram)}\\\"\"\n",
        "        f\" is not part of the dataset.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH5is1_RLnuS"
      },
      "source": [
        "When you run the cell above, the model generates a probability distribution for the next token. Some tokens will have higher probabilities than others, meaning they are more likely to be chosen as the next token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIr08bp1ZtVo"
      },
      "source": [
        "Here are a few likely observations :\n",
        "\n",
        "1. The Gemma model is able to assign probabilities to more tokens than the trigram model (which fails to assign probabilities to the next token for many prompts).\n",
        "2. The most probable token will usually be a common word that fits the context of the sentence (e.g., \"food\" after the prompt \"Jide was hungry so she went looking for\").\n",
        "3. The model might suggest words that seem plausible but do not carry a lot of information like \"a\" or \"something\".\n",
        "4. You might notice some tokens have low probabilities, meaning the model considers them less likely to fit but does not completely rule them out, like \"work\", \"help\", or \"Banku\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy9BsjpKLuhE"
      },
      "source": [
        "### Investigate the context-sensitivity of the two models\n",
        "\n",
        "What happens to the probability distribution if the context is changed? Generate a next token prediction for the prompt \"Jide was thirsty so she went looking for\" and consider both the generation and the distribution over the next token. Then, compare the distributions to the distributions of the original prompt \"Jide was hungry so she went looking for.\" For which model do the distributions change more?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TFCkKEfAMFG-"
      },
      "outputs": [],
      "source": [
        "# @title Predict the next token and visualize the distributions\n",
        "\n",
        "prompt = \"Jide was thirsty so she went looking for\"  # @param {type: \"string\"}\n",
        "\n",
        "output_text_transformer, next_token_logits, tokenizer = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt, max_new_tokens=1, loaded_model=gemma_model\n",
        "    )\n",
        ")\n",
        "\n",
        "output_text_ngram = trigram_model.generate(1, prompt)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
        "output_text_ngram = trigram_model.generate(1, prompt)\n",
        "\n",
        "print(f\"Generation by trigram model:\\n{output_text_ngram}\")\n",
        "\n",
        "display(HTML(\"<h3>Gemma-1B</h3>\"))\n",
        "\n",
        "# Visualize the Gemma-1B probabilities.\n",
        "visualizations.plot_next_token(next_token_logits, prompt=prompt, tokenizer=tokenizer)\n",
        "\n",
        "display(HTML(\"<h3>Trigram model</h3>\"))\n",
        "\n",
        "# Visualize the trigram probabilities.\n",
        "context_ngram = tuple(prompt.split(\" \")[-2:])\n",
        "if context_ngram in trigram_model.probabilities:\n",
        "    visualizations.plot_next_token(\n",
        "        trigram_model.probabilities[context_ngram], prompt=prompt\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"The trigram model does not make any predictions for the prompt\"\n",
        "        f\" \\\"{prompt}\\ since the bigram \\\"{' '.join(context_ngram)}\\\"\"\n",
        "        f\" is not part of the dataset.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxL_TBTMTI8u"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "When running the transformer model with prompts like `\"Jide was thirsty so she went looking for\"`, you might notice certain patterns in the predicted next tokens. For instance, you may see drink-related words like \"water\" suggested more often. This is because the transformer model is **context-sensitive** and associates tokens related to hunger with tokens like \"food\", and tokens related to thirst with tokens like \"water\" based on the entire prompt.\n",
        "\n",
        "The distribution over the next token as predicted by the trigram model, on the other hand, did not change at all. This is because this model's predictions are only based on the last bigram \"looking for\" which is the same across the two prompts.\n",
        "\n",
        "This highlights a key shortcoming of n-gram models: They have very short context windows and are unable to take information into account that does not appear at the very end of the context. Transformer models, on the other hand, usually have a context window of hundreds or thousands of tokens and can therefore provide much more context-sensitive answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6dyJeR3Zz3e"
      },
      "source": [
        "### Generate sequences\n",
        "\n",
        "In the previous activities, you have focused on predicting only one token. However, usually, you will use language models to predict entire sequences. In this activity, you will compare the outputs of the Gemma-1B model to the outputs of the trigram model when predicting longer sequences.\n",
        "\n",
        "\n",
        "Change `num_tokens_to_generate` to set the number of tokens to generate so that the generations are longer sequences. Generate continuations for several prompts. Then compare the generations of the two models along the four evaluation criteria mentioned previously:\n",
        "1. Fluency\n",
        "2. Coherence\n",
        "3. Relevance\n",
        "4. Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wPVIFr68Z1vT"
      },
      "outputs": [],
      "source": [
        "# @title Generate sequences\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
        "\n",
        "num_tokens_to_generate = 50  # @param {type: \"number\"}\n",
        "\n",
        "(output_text_transformer, next_token_logits, tokenizer) = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt, max_new_tokens=num_tokens_to_generate, loaded_model=gemma_model\n",
        "    )\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
        "\n",
        "output_text_ngram = trigram_model.generate(num_tokens_to_generate, prompt)\n",
        "print(f\"Generation by trigram model:\\n{output_text_ngram}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fggmt1p4Xn1y"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "You likely made some of the following observations:\n",
        "\n",
        "1. **Fluency**: The Gemma-1B model tends to be much more fluent than the trigram model and generates texts that follow the rules of English. The trigram model tends to generate sequences where short phrases are fluent but globally there may be many mistakes. This is again caused by the small context window of the n-gram model.\n",
        "2. **Coherence**: While not always perfect, the Gemma-1B model also produces more coherent generations than the trigram model. While you may sometimes observe generations by both models that do not make any sense or go off topic, this tends to be much less of a problem with transformer models such as Gemma.\n",
        "3. **Relevance**: The trigram model is rarely able to produce relevant responses. This becomes particularly pronounced when you use a question as a prompt. The Gemma-1B model again performs much better against this criterion.\n",
        "4. **Bias**: Language models tend to suffer from similar biases in the data that they were trained on. For example, if you compare several generations of the models for the prompts \"The nurse went to university in Ethiopia.\" and \"The doctor went to university in Ethiopia.\", you will likely observe that the Gemma-1B model continues  more often with female pronouns such as \"she\" or \"her\" when talking about a nurse, and more often with male pronouns such as \"he\" and \"him\" when talking about a doctor. This is because there tend to be many more texts about male doctors and female nurses than the other way round. When models are trained from generally available texts, they likely also learn such undesirable patterns. The trigram model tends to suffer from similar biases but since it rarely generates coherent responses it cannot really be used in practice (as you may have noticed it generates continuations about coffee for these two prompts).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGF0ds-NiBYt"
      },
      "source": [
        "#### Diversity of generations\n",
        "\n",
        "You likely noticed that the output of both models tend to change often when you run one of the cells above, even with the same prompt. As discussed in the previous labs, this is because the model uses a probability distribution to pick the next token, which introduces a level of stochasticity (randomness) into the prediction. As mentioned before, this variability helps the model generate more diverse and creative outputs. It allows users to regenerate a different response if they are not satisfied with the initial one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUs4waCjOO_f"
      },
      "source": [
        "### Controlling the model output\n",
        "\n",
        "Sometimes it may be desirable to make the output **deterministic** and to always choose the token with the highest probability. This is known as **greedy sampling**.  \n",
        "\n",
        "The following cell provides you with the option to switch between random sampling and greedy sampling. Depending on whether you set `sampling_mode` to `greedy` or `random`, you should get a deterministic or a random output respectively.\n",
        "\n",
        "Run the following cell multiple times with `sampling_mode` set to `random` and then with `sampling_mode` set to `greedy`. When it is set to `random` you should get different outputs most of the time, when it is set to `greedy` re-running the cell should always lead to the same output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rv7PzJFQdXiP"
      },
      "outputs": [],
      "source": [
        "# @title Random vs. deterministic generations\n",
        "prompt = \"Jide was thirsty so she went looking for\"  # @param {type: \"string\"}\n",
        "\n",
        "num_tokens_to_generate = 50  # @param {type: \"number\"}\n",
        "\n",
        "sampling_mode = \"random\"  # @param {type: \"string\", values:[\"random\", \"greedy\"]}\n",
        "\n",
        "\n",
        "(output_text_transformer, next_token_logits, tokenizer) = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt,\n",
        "        max_new_tokens=num_tokens_to_generate,\n",
        "        loaded_model=gemma_model,\n",
        "        sampling_mode=sampling_mode,\n",
        "    )\n",
        ")\n",
        "clear_output()\n",
        "\n",
        "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
        "\n",
        "output_text_ngram = trigram_model.generate(\n",
        "    num_tokens_to_generate, prompt, sampling_mode=sampling_mode\n",
        ")\n",
        "print(f\"Generation by trigram model:\\n{output_text_ngram}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV6Qd59qFuq-"
      },
      "source": [
        "#### Balancing creativity and consistency\n",
        "\n",
        "Sampling from a probability distribution allows the model to explore a range of possible next tokens, fostering creativity and generating varied outputs. This approach contrasts with always picking the token with the highest probability, which focuses on the most likely next token, as you have experienced above.\n",
        "\n",
        "Different applications require different settings for this balance. For creative tasks such as generating stories, sampling from the probability distribution is ideal. This is because it allows the model to explore various possibilities and produce more imaginative results.\n",
        "\n",
        "If accuracy, consistency, and reliability are important for your use case, it is better to always choose the token with the highest probability. There are also methods that allow for a balance between these two approaches. You will learn more about these in later courses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ6QjhZ5NK1i"
      },
      "source": [
        "## Takeaways\n",
        "\n",
        "You have now directly compared the generations of a trigram model and a transformer model and have observed many differences. These comparisons highlighted contrasts in terms of fluency, coherence and relevance between the two models. While the n-gram model often generated word salads or failed to generate a continuation at all, the transformer model generally generated quite reasonable responses (though sometimes they may have not been entirely perfect either).\n",
        "\n",
        "Note that this comparison was stacked against the n-gram model. That is because the difference between the trigram model and the Gemma-1B model, which were both trained the Africa Galore dataset, is not only one of implementation. The Gemma-1B model has also been trained on a very large dataset. In comparison, the trigram model has only been trained on the paragraphs in the Africa Galore dataset. That being said, even if you had trained the n-gram model on as much data as the Gemma-1B model, the transformer model would have still performed much better.\n",
        "\n",
        "There are two primary reasons for this:\n",
        "- Transformers have much larger context windows and can therefore consider the information of tokens that are further away from the token to be generated. N-gram models, on the other hand, only have a context window of $n-1$. So in the case of the trigram model, the model only considered the last two tokens for making predictions.\n",
        "- Transformers are based on neural networks that can learn **sophisticated** and **abstract** patterns. As you will learn more in later courses, neural networks can learn much more sophisticated patterns, and for example can learn that *food* and *snack* have related meanings. This allows the model to abstract away from specific words and learn more general patterns about language, which in return allows it to generate more diverse and more coherent responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU-cx5lKF4OS"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This is the end of the **Compare N-Gram Models and Transformer Language Models** lab.\n",
        "\n",
        "In this lab, you:\n",
        "\n",
        "- Experienced what generations of transformer models look like and how they compare to the generations of n-gram models.\n",
        "\n",
        "- Tried different prompts and observed how the model predictions and their probabilities changed (or did not change) based on the context.\n",
        "\n",
        "- Visualized the probability distributions over the next token to gain a deeper understanding of the model behavior when randomly sampling the next token.\n",
        "\n",
        "- Compared the models' abilities in generating longer sequences of text and explored how you can make the generations deterministic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Db1fVkwwIb"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Kamath et al. (Gemma Team). 2025. Gemma 3 Technical Report. Google DeepMind, London. arXiv:2503.19786. Retrieved from https://arxiv.org/pdf/2503.19786."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}